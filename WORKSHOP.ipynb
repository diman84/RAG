{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents from GIT Repository\n",
    "Langchain `DocumentLoaders` load data into the standard LangChain `Document` format. We can use components provided by langchain or any custom loader implementing `DocumentLoader` interface.\n",
    "\n",
    "Some of available loaders in [langchain_community.document_loaders](https://python.langchain.com/docs/integrations/document_loaders/):\n",
    "- **Webpages**: allow you to load webpages.\n",
    "- **PDFs**: allow you to load PDF documents.\n",
    "- **Cloud Providers**: allow you to load documents from your favorite cloud providers.\n",
    "- **Tools**: allow you to load data from commonly used tools like Slack, Quip, Github and more..\n",
    "- **Local**: allow you to load data from your local file system.\n",
    "\n",
    "In order to load files from GIT repository we can use `GitLoader` community package that will allow to load reposiitory data, where each document would represent one file in the repository.\n",
    "\n",
    "We can also utilize `GitPython` package to clone repository and retreive another useful information.\n",
    "\n",
    "```\n",
    "> pip install --upgrade --quiet  GitPython\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from git import Repo\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "jsLoader = GitLoader(repo_path=\"./client-server-example/\", branch='master', file_filter=lambda file_path: file_path.endswith(\".js\"))\n",
    "mdLoader = GitLoader(repo_path=\"./client-server-example/\", branch='master', file_filter=lambda file_path: file_path.endswith(\".md\"))\n",
    "defaultLoader = GitLoader(repo_path=\"./client-server-example/\", branch='master', file_filter=lambda file_path: not file_path.endswith(\".md\") and not file_path.endswith(\".js\"))\n",
    "\n",
    "jsData = jsLoader.load()\n",
    "mdData = mdLoader.load()\n",
    "otherData = defaultLoader.load()\n",
    "\n",
    "print(len(jsData))\n",
    "print(len(mdData))\n",
    "print(len(otherData))\n",
    "\n",
    "#print(mdData[0])\n",
    "#print(jsData[1])\n",
    "#print(otherData[3])\n",
    "\n",
    "repo = Repo(\"./client-server-example/\")\n",
    "def get_changed_files():\n",
    "    changed_files = []\n",
    "    diff_index = repo.index.diff(None)\n",
    "    for diff_item in diff_index:\n",
    "        changed_files.append(diff_item.a_path)\n",
    "\n",
    "    return changed_files\n",
    "\n",
    "def get_commits_from_branch(branch_name):\n",
    "    branch = next(filter(lambda b: b.name == branch_name, repo.branches), None)\n",
    "    if (branch is not None):        \n",
    "        commits = list(branch.commit.iter_items(repo, branch.commit))    \n",
    "        return [commit.message for commit in commits]\n",
    "    ref = next(filter(lambda b: b.name == f\"origin/{branch_name}\", repo.remote().refs), None)\n",
    "    if ref is not None:        \n",
    "        commits = list(repo.iter_commits(ref))    \n",
    "        return [commit.message for commit in commits]    \n",
    "    return []\n",
    "\n",
    "def load_commits() -> Iterator[Document]:\n",
    "    for commit in repo.iter_commits():\n",
    "        metadata = {\n",
    "            \"commit_author_name\": commit.author.name,\n",
    "            \"commit_author_email\": commit.author.email,\n",
    "            \"commit_authored_datetime\": commit.authored_datetime,\n",
    "            \"commit_committed_datetime\": commit.committed_datetime,\n",
    "        }\n",
    "        yield Document(page_content=commit.message, metadata=metadata)\n",
    "\n",
    "commitsData = list(load_commits())\n",
    "\n",
    "print(len(commitsData))\n",
    "#print(commitsData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Split documents into chunks\n",
    "\n",
    "There are different ways to split a document into chunks, the most challenging part that we need to consider is to split the document into meaningful chunks, and it can strongly depend on the document type and the use case.\n",
    "\n",
    "In our case there are 3 types of documents we can consider:\n",
    "  - **Text documents**: We can split the text into paragraphs, sentences, or words.\n",
    "  - **Code documents**: We can split the code into functions, classes, or lines.\n",
    "  - **Git data**: We can split the git data into commits, files, or lines.\n",
    "\n",
    "Splitters supported by [langchain.text_splitter](https://python.langchain.com/v0.2/api_reference/text_splitters/index.html) that we can use:\n",
    "  - **RecursiveCharacterTextSplitter**: Implementation of splitting text that recursively looks at characters.\n",
    "  - **MarkdownHeaderTextSplitter**: Implementation of splitting markdown files based on specific headers.\n",
    "  - **TokenTextSplitter**: Implementation of splitting text that looks at tokens.\n",
    "  - **SentenceTransformersTokenTextSplitter**: It is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.\n",
    "  - **RecursiveJsonSplitter**: Implementation of splitting text that looks at characters. Recursively tries to split by different characters to find the one that works.\n",
    "  - **Language**: for CPP, Python, Ruby, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, Language, TokenTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "md_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "token_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "sentence_splitter = SentenceTransformersTokenTextSplitter()\n",
    "\n",
    "js_docs = js_splitter.split_documents(jsData)\n",
    "md_docs = md_splitter.split_documents(mdData)\n",
    "other_docs = token_splitter.split_documents(otherData)\n",
    "commit_docs = sentence_splitter.split_documents(commitsData)\n",
    "\n",
    "\n",
    "print(len(commit_docs))\n",
    "print(len(js_docs))\n",
    "print(len(md_docs))\n",
    "print(len(other_docs))\n",
    "\n",
    "#print(js_docs[0])\n",
    "#print(md_docs[0])\n",
    "#print(other_docs[0])\n",
    "#print(commit_docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding vectors\n",
    "\n",
    "Embeddings are essential for LLM tasks, they are high-dimensional vectors that capture the semantic meaning of tokens in chunks. We will use them for document corpus and for the query to search for relevant chunks that will be included into the context to generate completions.\n",
    "\n",
    "From this point we need to find a structure of contextual query to find proper documents for suggestion. Lets assume it will be changes in the code of current document. We can use `GitPython` to get the changes in the repository and use them as a query.\n",
    "\n",
    "To understand embeddings and how they aligned you can play with text embeddings in Cohere Playground (https://dashboard.cohere.com/playground/embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "openAIEmb = OpenAIEmbeddings(api_key=\"KEY\")\n",
    "ollamaEmb = OllamaEmbeddings(model=\"starcoder2:3b\")\n",
    "hfEmb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2') #sentence-transformers/all-mpnet-base-v2\n",
    "sbertEmb = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "(js_embeddings, md_embeddings) = await asyncio.gather(\n",
    "    openAIEmb.aembed_documents([doc.page_content for doc in js_docs]),\n",
    "    ollamaEmb.aembed_documents([doc.page_content for doc in md_docs])\n",
    "    )\n",
    "\n",
    "commit_embeddings = sbertEmb.encode([doc.page_content for doc in commit_docs], convert_to_tensor=True) # hfEmb.embed_documents(md_docs)\n",
    "\n",
    "print(len(js_embeddings))\n",
    "print(len(md_embeddings))\n",
    "print(len(commit_embeddings))\n",
    "\n",
    "# print vector dimensions\n",
    "print(len(js_embeddings[0]))\n",
    "print(len(md_embeddings[0]))\n",
    "print(commit_embeddings.shape)\n",
    "\n",
    "# understanding embeddings\n",
    "# https://dashboard.cohere.com/playground/embed\n",
    "# mother, father, aunt, uncle\n",
    "plur = numpy.subtract(openAIEmb.embed_query(\"students\"), openAIEmb.embed_query(\"student\"))\n",
    "\n",
    "print(numpy.dot(plur, openAIEmb.embed_query(\"cat\")))\n",
    "print(numpy.dot(plur, openAIEmb.embed_query(\"cats\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to Vector DB\n",
    "Vector DBs are very diverse, they support different types of embedding models and different types of search and API capabilities. Most of them support `langchain` models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple In-Memory\n",
    "# Chroma\n",
    "# Faiss\n",
    "# Qdrant\n",
    "\n",
    "import faiss\n",
    "import uuid\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "in_memory_vector_store = InMemoryVectorStore(openAIEmb)\n",
    "\n",
    "faiss_vec_dim = commit_embeddings.shape[1] # vector size of size of sentence-BERT embeddings\n",
    "faiss_index = faiss.IndexFlatL2(faiss_vec_dim)\n",
    "faiss_index.train(commit_embeddings)\n",
    "faiss_vector_store = FAISS(\n",
    "    embedding_function=hfEmb,\n",
    "    index=faiss_index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"text_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE), # 1536 - vector size of 'text-embedding-ada-002' OpenAI embeddings\n",
    ")\n",
    "client.create_collection(\n",
    "    collection_name=\"text_collection_hf\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE), # 768 - vector size of SBERT embeddings\n",
    ")\n",
    "client.create_collection(\n",
    "    collection_name=\"code_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE), # 1536 - vector size of 'text-embedding-ada-002' OpenAI embeddings\n",
    ")\n",
    "client.create_collection(\n",
    "    collection_name=\"code_collection_sc\",\n",
    "    vectors_config=VectorParams(size=3072, distance=Distance.COSINE), # 3072 - default vector size of starcoder2 embeddings\n",
    ")\n",
    "client.create_collection(\n",
    "    collection_name=\"docs_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE), # 1536 - vector size of 'text-embedding-ada-002' OpenAI embeddings\n",
    ")\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"docs_collection\",\n",
    "    embedding=openAIEmb, #ollamaEmb\n",
    ")\n",
    "\n",
    "# Add documents to the vector stores\n",
    "in_memory_vector_store.add_documents(documents=js_docs)\n",
    "\n",
    "qdrant_vector_store.add_documents(documents=md_docs)\n",
    "\n",
    "uuids = [str(uuid.uuid4()) for _ in range(len(commit_docs))]\n",
    "faiss_vector_store.add_documents(documents=commit_docs, ids=uuids)\n",
    "# FAISS.from_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search vectorized data\n",
    "In `Langchain`, components that are responsible to returns documents given an unstructured query ara called `Retreivers`, responsible for finding the most relevant documents along with its \"relativity\" to the query if possible.\n",
    "\n",
    "Retreivers, availabl for `langchain` integration can be found here https://python.langchain.com/docs/integrations/retrievers/ along with the usage guide https://python.langchain.com/docs/how_to/#retrievers\n",
    "\n",
    "We will start from vectorized data search capabilities based on vector store-backed retriever and search specific to certain types of stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic similarity SBERT (sbert api)\n",
    "# similarity search by query and by vector\n",
    "# use retreivers without Vector DB (KNN, etc.)\n",
    "# FAISS similarity, index search (faiss api)\n",
    "# Qdrant MMR\n",
    "# Chroma\n",
    "\n",
    "import torch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from sentence_transformers import SimilarityFunction\n",
    "from sentence_transformers import util\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from tree_sitter_languages import get_parser\n",
    "\n",
    "current_diff = repo.git.diff()\n",
    "#print(current_diff)\n",
    "\n",
    "js_retriever = in_memory_vector_store.as_retriever(k=3) # similarity, mmf, scored similarity\n",
    "# js_retriever = in_memory_vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}, k=1)\n",
    "similar_js_docs = js_retriever.invoke(current_diff)\n",
    "print(\"similar js docs\")\n",
    "print(similar_js_docs)\n",
    "\n",
    "# summarize current diff to search info in commits\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"KEY\")\n",
    "summary_template = ChatPromptTemplate.from_template(\"Create short meaningful commit message of git diff.\\n{git_diff}\")\n",
    "# Try FewShotChatMessagePromptTemplate with example prompts\n",
    "summary_chain = summary_template | chat | StrOutputParser()\n",
    "summary = summary_chain.invoke({\"git_diff\":current_diff})\n",
    "print(f\"summary: {summary}\")\n",
    "\n",
    "# similarity search by query, vector and score\n",
    "summary_embeddings = hfEmb.embed_query(summary)\n",
    "similar_commits = faiss_vector_store.similarity_search_by_vector(summary_embeddings) # ANN search\n",
    "print(\"FAISS similar commits\")\n",
    "print(similar_commits)\n",
    "\n",
    "similar_commits_score = faiss_vector_store.similarity_search_with_score(summary, k=2)\n",
    "print(\"FAISS similar commits with score\")\n",
    "for (doc, score) in similar_commits_score:\n",
    "    print(f\"[{score}] {doc.page_content}\")\n",
    "\n",
    "# specialized search with retreiver\n",
    "# BM25, TF-IDF, KNN, SVM, etc.\n",
    "remote_commits = get_commits_from_branch(\"with_router\")\n",
    "bm25_results = BM25Retriever.from_texts(remote_commits).invoke(summary)\n",
    "print(\"BM25 similar commits\")\n",
    "print(bm25_results)\n",
    "\n",
    "# SBERT smantic search (ENN search)\n",
    "# Select model depending on Symmetric vs Asymmetric Semantic Search\n",
    "sbertEmb.similarity_fn_name = SimilarityFunction.DOT_PRODUCT # SimilarityFunction.COSINE, etc.\n",
    "# model processing options can be cpu or gpu\n",
    "commits_emb = sbertEmb.encode(remote_commits, convert_to_tensor=True)\n",
    "summary_emb = sbertEmb.encode([summary], convert_to_tensor=True)\n",
    "commit_similarities = sbertEmb.similarity(summary_emb, commits_emb)[0] # pairwise_similarity\n",
    "scores, indices = torch.topk(commit_similarities, k=2)\n",
    "print(\"SBERT similar commits\")\n",
    "for score, idx in zip(scores, indices):\n",
    "    print(f\"[{score:.4f}] {remote_commits[idx]}\".rstrip())\n",
    "\n",
    "# speed optimization\n",
    "commits_emb_gpu = commits_emb.to(\"cuda\")\n",
    "commits_emb_gpu = util.normalize_embeddings(commits_emb_gpu) # ENN\n",
    "summary_emb_gpu = summary_emb.to(\"cuda\")\n",
    "summary_emb_gpu = util.normalize_embeddings(summary_emb_gpu)\n",
    "hits = util.semantic_search(summary_emb_gpu, commits_emb_gpu, score_function=util.dot_score)\n",
    "print(\"GPU similar commits\")\n",
    "print(hits)\n",
    "\n",
    "# Qdrant MMR\n",
    "md_results = qdrant_vector_store.max_marginal_relevance_search(current_diff, k=3)\n",
    "print(\"Qdrant MMR similar md docs\")\n",
    "print(md_results)\n",
    "\n",
    "# FAISS specific search with performance improvements\n",
    "new_index = faiss.IndexIVFFlat(faiss_index, commit_embeddings.shape[1], 2)\n",
    "new_index.train(commit_embeddings)\n",
    "new_index.add(commit_embeddings)\n",
    "#d,i = new_index.search(summary_emb, k=1)\n",
    "print(\"FAISS specific search\")\n",
    "#print(i)\n",
    "\n",
    "# understanding relevance, score\n",
    "qdrant_test_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"text_collection\",\n",
    "    embedding=openAIEmb\n",
    ")\n",
    "\n",
    "qdrant_test_store.add_texts([\"She deposited money at the bank.\",\"The boat was tied to the river bank.\"])\n",
    "\n",
    "bank_relevance = qdrant_test_store.similarity_search_with_relevance_scores(\"steep bank\")\n",
    "bank_score = qdrant_test_store.similarity_search_with_score(\"steep bank\")\n",
    "bank_mmr = qdrant_test_store.max_marginal_relevance_search(\"steep bank\")\n",
    "\n",
    "print(bank_relevance)\n",
    "print(bank_score)\n",
    "print(bank_mmr)\n",
    "\n",
    "bank_relevance = qdrant_test_store.similarity_search_with_relevance_scores(\"reliable bank\")\n",
    "bank_score = qdrant_test_store.similarity_search_with_score(\"reliable bank\")\n",
    "bank_mmr = qdrant_test_store.max_marginal_relevance_search(\"reliable bank\")\n",
    "\n",
    "print(bank_relevance)\n",
    "print(bank_score)\n",
    "print(bank_mmr)\n",
    "\n",
    "# code similarity\n",
    "sort = \"\"\"\n",
    "void f(int[] array) {\n",
    "    boolean swapped = true;\n",
    "    for (int i = 0; i < array.length && swapped; i++) {\n",
    "        swapped = false;\n",
    "        for (int j = 0; j < array.length - 1 - i; j++) {\n",
    "           if (array[j] > array[j+1]) {\n",
    "               int temp = array[j];\n",
    "               array[j] = array[j+1];\n",
    "               array[j+1]= temp;\n",
    "               swapped = true;\n",
    "           }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "stddev = \"\"\"\n",
    "const f = (trials, len) => {\n",
    "    let sum = 0;\n",
    "    let squ = 0.0;\n",
    "\n",
    "    for (let i = 0; i < len; i++) {\n",
    "        let d = trials[i];\n",
    "        sum += d;\n",
    "        squ += d * d;\n",
    "    }\n",
    "\n",
    "    let x = squ - (sum * sum) / len;\n",
    "    let res = Math.sqrt(x / (len - 1));\n",
    "\n",
    "    return Math.floor(res);\n",
    "};\n",
    "\"\"\"\n",
    "\n",
    "indexOf = \"\"\"\n",
    "function f(array, value) {\n",
    "    for (let i = 0; i < array.length; i++) {\n",
    "        if (array[i] === value) {\n",
    "            return i;\n",
    "        }\n",
    "    }\n",
    "    return -1;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sql_query = \"SELECT * FROM customers WHERE city = 'Berlin'\"\n",
    "\n",
    "qdrant_code_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"code_collection_sc\",\n",
    "    embedding=ollamaEmb\n",
    ")\n",
    "\n",
    "qdrant_code_store.add_texts([sort, stddev, indexOf])\n",
    "\n",
    "qdrant_asm_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"code_collection\",\n",
    "    embedding=openAIEmb\n",
    ")\n",
    "\n",
    "sort_relevance = qdrant_code_store.similarity_search_with_relevance_scores(\"sort\")\n",
    "sort_score = qdrant_code_store.similarity_search_with_score(\"sort\")\n",
    "\n",
    "print(sort_relevance)\n",
    "print(sort_score)\n",
    "\n",
    "sql_relevance = qdrant_code_store.similarity_search_with_relevance_scores(sql_query)\n",
    "sql_score = qdrant_code_store.similarity_search_with_score(sql_query)\n",
    "\n",
    "print(sort_relevance)\n",
    "print(sort_score)\n",
    "\n",
    "# code similarity with tree-sitter AST\n",
    "js_parser = get_parser('javascript')\n",
    "sql_parser = get_parser('sql')\n",
    "java_parser = get_parser('java')\n",
    "\n",
    "ts_query = sql_parser.parse(sql_query.encode()).root_node.sexp()\n",
    "\n",
    "ts_indexOf = js_parser.parse(indexOf.encode()).root_node.sexp()\n",
    "ts_stdev = js_parser.parse(stddev.encode()).root_node.sexp()\n",
    "ts_sort = java_parser.parse(sort.encode()).root_node.sexp()\n",
    "\n",
    "qdrant_code_store.add_documents([\n",
    "    Document(page_content=ts_sort, metadata={'source':'ts_sort', 'source_code': sort, 'language':'java'}), \n",
    "    Document(page_content=ts_stdev, metadata={'source':'ts_stdev', 'source_code': stddev, 'language':'js'}), \n",
    "    Document(page_content=ts_indexOf, metadata={'source':'ts_indexOf', 'source_code': indexOf, 'language':'js'})])\n",
    "\n",
    "ts_relevance = qdrant_code_store.similarity_search_with_relevance_scores(ts_query)\n",
    "ts_score = qdrant_code_store.similarity_search_with_score(ts_query)\n",
    "print(ts_relevance)\n",
    "print(ts_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add filtering by metadata\n",
    "Filtering by metadata is a common use case in search engines, we can utilize any specific data store or database api to filter the data, or we can use `langchain` retrievers capabilities to filter the data based on metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with self query filter\n",
    "# time-weighted retriever\n",
    "# get doc from shunks\n",
    "\n",
    "#from langchain_chroma import Chroma\n",
    "\n",
    "file_list = get_changed_files()\n",
    "print(file_list)\n",
    "other_docs_store = InMemoryVectorStore.from_documents(other_docs, openAIEmb) # Chroma.from_documents(other_docs, openAIEmb)\n",
    "other_results = other_docs_store.as_retriever(search_kwargs= {\"filter\": {\"file_name\": {\"$in\": file_list}}}).invoke(current_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use text search approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant Hybrid\n",
    "# Git search and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use reranking for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT\n",
    "# Cohere reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced search options\n",
    "Logical and Semantic routing (different data source based on input) - https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb \n",
    "Query Analysis (to apply fiter based on input) - https://python.langchain.com/docs/tutorials/query_analysis/#query-analysis \n",
    "MultiQuery (LLM to analyze input and generate query) - https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logical and Semantic routing for different doc types\n",
    "# Multi-Query query ewith retreiver\n",
    "# Timestamp-weighted (based on commits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "The final prompt will be a combination of the context and the query. Its structure is strongly dependent on the use case, the data we have and LLM that is used. For example for completions we can use special purpose  **Stable Code** models that will require quite strict structure of the prompt. For general purpose models we can follow some common suggestions:\n",
    "  - Be specific (dierectly specify the language, purpose, context, desired output)\n",
    "  - Use examples (provide examples of the code suggestions, the data)\n",
    "  - Use structured output if needed and possible\n",
    "  - Provide system instructions if possible, use role-based prompts\n",
    "\n",
    "### Defining Context\n",
    "When defining context the main limitation is number of tokens we can provide to the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG\n",
    "https://microsoft.github.io/graphrag/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
